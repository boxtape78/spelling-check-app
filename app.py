{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d290537-d190-4286-bd12-f1bf5db08463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import io\n",
    "import zipfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# --- NLTK ÏÑ§Ï†ï ---\n",
    "@st.cache_resource\n",
    "def setup_nltk():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "setup_nltk()\n",
    "\n",
    "# --- ÌïµÏã¨ Î°úÏßÅ ---\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def is_candidate_word(tok):\n",
    "    return isinstance(tok, str) and tok.isalpha() and len(tok) > 2 and not tok.isupper()\n",
    "\n",
    "def count_real_words(text):\n",
    "    return sum(1 for t in tokenize_text(text) if is_candidate_word(t))\n",
    "\n",
    "def analyze_and_correct(text, spell):\n",
    "    tokens = tokenize_text(text)\n",
    "    candidate_indices = [i for i, t in enumerate(tokens) if is_candidate_word(t)]\n",
    "    candidate_words = [tokens[i].lower() for i in candidate_indices]\n",
    "    misspelled_set = spell.unknown(candidate_words)\n",
    "\n",
    "    corrections = {}\n",
    "    error_count = 0\n",
    "    misspelled_list = []\n",
    "    \n",
    "    detok = TreebankWordDetokenizer()\n",
    "    corrected_tokens = list(tokens)\n",
    "\n",
    "    for idx, lw in zip(candidate_indices, candidate_words):\n",
    "        if lw in misspelled_set:\n",
    "            surface = tokens[idx]\n",
    "            suggestion = spell.correction(lw)\n",
    "            if not suggestion: \n",
    "                suggestion = surface\n",
    "            \n",
    "            corrections.setdefault(surface, suggestion)\n",
    "            error_count += 1\n",
    "            misspelled_list.append(surface)\n",
    "            \n",
    "            if surface.istitle():\n",
    "                final_word = suggestion.capitalize()\n",
    "            elif surface.isupper():\n",
    "                final_word = suggestion.upper()\n",
    "            else:\n",
    "                final_word = suggestion\n",
    "\n",
    "            corrected_tokens[idx] = final_word\n",
    "\n",
    "    corrected_text = detok.detokenize([t if isinstance(t, str) else \"\" for t in corrected_tokens])\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(misspelled_list)\n",
    "    pos_profile = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "    return corrected_text, corrections, error_count, pos_profile\n",
    "\n",
    "# --- Streamlit ÌôîÎ©¥ Íµ¨ÏÑ± ---\n",
    "st.title(\"üìù Spelling Checker & Profiler\")\n",
    "st.markdown(\"Ïó¨Îü¨ Í∞úÏùò `.txt` ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÎ©¥ Ïä§Ìé†ÎßÅÏùÑ ÍµêÏ†ïÌïòÍ≥† Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\"Í≤ÄÏÇ¨Ìï† ÌÖçÏä§Ìä∏ ÌååÏùºÎì§ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî\", type=\"txt\", accept_multiple_files=True)\n",
    "\n",
    "if uploaded_files:\n",
    "    if st.button(\"Ïä§Ìé†ÎßÅ Í≤ÄÏÇ¨ ÏãúÏûë\"):\n",
    "        spell = SpellChecker()\n",
    "        zip_buffer = io.BytesIO()\n",
    "        error_summary = []\n",
    "        all_pos_profile = Counter()\n",
    "        \n",
    "        with zipfile.ZipFile(zip_buffer, \"w\") as zf:\n",
    "            progress_bar = st.progress(0)\n",
    "            \n",
    "            for i, uploaded_file in enumerate(uploaded_files):\n",
    "                original_text = uploaded_file.getvalue().decode(\"utf-8\", errors=\"replace\")\n",
    "                filename = uploaded_file.name\n",
    "                \n",
    "                corrected_text, corrections, err_count, pos_profile = analyze_and_correct(original_text, spell)\n",
    "                all_pos_profile.update(pos_profile)\n",
    "                \n",
    "                zf.writestr(f\"corrected_{filename}\", corrected_text)\n",
    "                \n",
    "                total_words = count_real_words(original_text)\n",
    "                error_rate = (err_count / total_words * 100) if total_words > 0 else 0\n",
    "                error_summary.append([filename, total_words, err_count, f\"{error_rate:.2f}%\"])\n",
    "                \n",
    "                progress_bar.progress((i + 1) / len(uploaded_files))\n",
    "\n",
    "            csv_buffer = io.StringIO()\n",
    "            writer = csv.writer(csv_buffer)\n",
    "            writer.writerow(['Filename', 'Total Words', 'Error Count', 'Error Rate'])\n",
    "            writer.writerows(error_summary)\n",
    "            zf.writestr(\"summary_report.csv\", csv_buffer.getvalue())\n",
    "            \n",
    "            txt_report = \"Total POS Error Profile:\\n\"\n",
    "            for tag, count in all_pos_profile.most_common():\n",
    "                txt_report += f\"{tag}: {count}\\n\"\n",
    "            zf.writestr(\"pos_analysis_report.txt\", txt_report)\n",
    "\n",
    "        st.success(\"‚úÖ Î∂ÑÏÑù ÏôÑÎ£å!\")\n",
    "        st.download_button(\n",
    "            label=\"Í≤∞Í≥º ÌååÏùº Îã§Ïö¥Î°úÎìú (ZIP)\",\n",
    "            data=zip_buffer.getvalue(),\n",
    "            file_name=\"spelling_check_results.zip\",\n",
    "            mime=\"application/zip\"\n",
    "        )\n",
    "        st.write(\"### üìä Ï†ÑÏ≤¥ Ïò§Î•ò ÌíàÏÇ¨ ÌÜµÍ≥Ñ\")\n",
    "        st.write(all_pos_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc895b1-dd69-4cd2-84b9-cd9c38a200a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
