import streamlit as st
from spellchecker import SpellChecker
import nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer
import io
import zipfile
import csv
from collections import Counter

# --- NLTK ì„¤ì • (ìºì‹±í•˜ì—¬ ì†ë„ í–¥ìƒ) ---
@st.cache_resource
def setup_nltk():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('taggers/averaged_perceptron_tagger')
    except LookupError:
        nltk.download('averaged_perceptron_tagger')

setup_nltk()

# --- í•µì‹¬ ë¡œì§ ---
def tokenize_text(text):
    return word_tokenize(text)

def is_candidate_word(tok):
    return isinstance(tok, str) and tok.isalpha() and len(tok) > 2 and not tok.isupper()

def count_real_words(text):
    return sum(1 for t in tokenize_text(text) if is_candidate_word(t))

def analyze_and_correct(text, spell):
    tokens = tokenize_text(text)
    candidate_indices = [i for i, t in enumerate(tokens) if is_candidate_word(t)]
    candidate_words = [tokens[i].lower() for i in candidate_indices]
    misspelled_set = spell.unknown(candidate_words)

    corrections = {}
    error_count = 0
    misspelled_list = []
    
    detok = TreebankWordDetokenizer()
    corrected_tokens = list(tokens)

    for idx, lw in zip(candidate_indices, candidate_words):
        if lw in misspelled_set:
            surface = tokens[idx]
            suggestion = spell.correction(lw)
            
            if not suggestion: 
                suggestion = surface
                
            corrections.setdefault(surface, suggestion)
            error_count += 1
            misspelled_list.append(surface)
            
            if surface.istitle():
                final_word = suggestion.capitalize()
            elif surface.isupper():
                final_word = suggestion.upper()
            else:
                final_word = suggestion

            corrected_tokens[idx] = final_word

    corrected_text = detok.detokenize([t if isinstance(t, str) else "" for t in corrected_tokens])
    
    pos_tags = nltk.pos_tag(misspelled_list)
    pos_profile = Counter(tag for word, tag in pos_tags)

    return corrected_text, corrections, error_count, pos_profile

# --- Streamlit í™”ë©´ êµ¬ì„± ---
st.title("ğŸ“ Spelling Checker & Profiler")
st.markdown("ì—¬ëŸ¬ ê°œì˜ `.txt` íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìŠ¤í ë§ì„ êµì •í•˜ê³  ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

uploaded_files = st.file_uploader("ê²€ì‚¬í•  í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”", type="txt", accept_multiple_files=True)

if uploaded_files:
    if st.button("ìŠ¤í ë§ ê²€ì‚¬ ì‹œì‘"):
        spell = SpellChecker()
        
        # ê²°ê³¼ë¬¼ì„ ëª¨ì„ ZIP íŒŒì¼ ìƒì„± ì¤€ë¹„
        zip_buffer = io.BytesIO()
        
        error_summary = []
        all_pos_profile = Counter()
        
        with zipfile.ZipFile(zip_buffer, "w") as zf:
            progress_bar = st.progress(0)
            
            for i, uploaded_file in enumerate(uploaded_files):
                # 1. íŒŒì¼ ì½ê¸°
                original_text = uploaded_file.getvalue().decode("utf-8", errors="replace")
                filename = uploaded_file.name
                
                # 2. ë¶„ì„ ë° êµì •
                corrected_text, corrections, err_count, pos_profile = analyze_and_correct(original_text, spell)
                all_pos_profile.update(pos_profile)
                
                # 3. êµì •ëœ íŒŒì¼ ZIPì— ì¶”ê°€
                zf.writestr(f"corrected_{filename}", corrected_text)
                
                # 4. ìš”ì•½ ì •ë³´ ì €ì¥
                total_words = count_real_words(original_text)
                error_rate = (err_count / total_words * 100) if total_words > 0 else 0
                error_summary.append([filename, total_words, err_count, f"{error_rate:.2f}%"])
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress((i + 1) / len(uploaded_files))

            # 5. CSV ë¦¬í¬íŠ¸ ìƒì„± ë° ZIPì— ì¶”ê°€
            csv_buffer = io.StringIO()
            writer = csv.writer(csv_buffer)
            writer.writerow(['Filename', 'Total Words', 'Error Count', 'Error Rate'])
            writer.writerows(error_summary)
            zf.writestr("summary_report.csv", csv_buffer.getvalue())
            
            # 6. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸(í’ˆì‚¬ ë¶„ì„ í¬í•¨) ìƒì„± ë° ZIPì— ì¶”ê°€
            txt_report = "Total POS Error Profile:\n"
            for tag, count in all_pos_profile.most_common():
                txt_report += f"{tag}: {count}\n"
            zf.writestr("pos_analysis_report.txt", txt_report)

        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
        st.download_button(
            label="ê²°ê³¼
